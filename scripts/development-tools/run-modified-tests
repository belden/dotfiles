#!/usr/bin/env perl

use strict;
use warnings;

use Getopt::Long;

my $retry_file;
my $failures = '/tmp/run-modified-tests.failures';
my $git_base_dir = '/home/dev/src/adama';

GetOptions(
  'quiet' => \(my $quiet = 0),
  'run!' => \(my $run = 1),
  'save!' => \(my $save = 1),
  'ret|retry' => sub {
    my $arg = shift;
    $retry_file = $failures;
    if (length($arg) && -f $arg) {
      $retry_file = $arg;
    }
  },
  'h|help' => sub { exit 0 * print(usage()) },
);

sub apply (&@) {
  my $code = shift;
  $code->() foreach @_;
  return @_;
}

chdir $git_base_dir;

my @tests;
if (@ARGV) {
  $save = 0;
  @tests = test_modules_to_module_runners(@ARGV);
} else {
  @tests = modified_tests($retry_file);
}

if (! $run) {
  print join(' ', @tests), "\n";
} else {
  my $verbose_flag = $quiet
    ? ''
    : ' --verbose';

  my $test_results = "/tmp/run-modified-tests.$$.out";
  system("(/usr/bin/env prove -Ilib -It/lib $verbose_flag @tests 2>&1) | tee $test_results");

  if ($save) {
    my @failed =
      apply { s/^(\S+).*$/$1/ }
      grep {
        $_ =~ /Test Summary Report/ ... $_ =~ /Result/
          ? $_ =~ /Wstat/
          : 0;
      }
      catfile($test_results);

    if (@failed) {
      chomp @failed;
      open my $fh, '+>', $failures or die "open $failures: $!\n";
      print($fh "$git_base_dir/$_\n") foreach @failed;
      close $fh;
      print <<"END";

Test failures, see $test_results

END
    }

    exit 1;
  }
}

exit 0;

sub catfile {
  my ($file) = @_;
  local @ARGV = ($file);
  (<>);
}

sub unzip (&@) {
  my $code = shift;
  my ($l, $r) = ([], []);
  foreach (@_) {
    my $ar = $code->() ? $l : $r;
    push @$ar, $_;
  }
  return ($l, $r);
}

sub modified_tests {
  my ($retry_file) = @_;

  if ($retry_file) {
    chomp(my @tests = catfile($retry_file));
    return @tests;
  } else {

    chomp(my @affected = `git status --porcelain`);
    my ($ts, $pms) =
      unzip { m{\.t$} }
      grep { m{t/lib/Adama/Test.*\.pm$} || m{\.t$} }
      apply { s/^..//; s/^\W+// }
      grep { $_ !~ /^\s*D/ }
      @affected;

    return (@$ts, test_modules_to_module_runners(@$pms));
  }
}

# turn a list of .pm files into a list of runners for those classes
sub test_modules_to_module_runners {
  my (@modules) = @_;

  my ($module_runners, $needs_work) = unzip { m{\.t$} } @modules;

  my $modules_regex = join '|', apply {
    s{t/lib/}{};
    s{/}{::}g;
    s{\.pm$}{};
  } @$needs_work;

  # let's assume that every .pm listed ->isa('Test::Class'), and we can
  # find its associated .t file by looking for a relevant ->runtests line
  my $command = qq{git grep -l -E '($modules_regex)->runtests';};
  chomp(my @got = `$command`);
  push @$module_runners, @got;

  # Uh-oh, we didn't find enough runner files. Maybe Belden has had his stinking
  # hands on the codebase...
  if (@$module_runners != @modules) {
    # see if there's a .t file that lives in the smae directory as this .pm
    push @$module_runners, grep { -f $_ } apply { s{\.pm$}{.t} } @modules;
  }

  if (@$module_runners != @modules) {
    die(<<"YIKES");
Yikes! couldn't find a runner for all modules!

given:
${\join "\n", map { "  $_" } @modules}

got:
${\join "\n", map { "  $_" } @$module_runners}
YIKES
  }
  return @$module_runners;
}

sub usage {
  return <<USAGE;
run-modified-tests [--quiet] [--norun] [--retry]

Find modified tests in the Adama codebase and run them.

Options:

  --run       default: --run
  --norun     don't actually run tests, just tell what we'd run

  --quiet     by default, 'prove' is run in --verbose mode.
              'run-modified-tests --quiet' will make your testrun
              a little more quiet.

  --retry     re-run failed tests from the last run-modified-tests testrun.
              The idea is that you 'run-modified-tests' once. Some tests will
              pass, others will fail. You begin working your way through the
              list of failures, making them pass, and periodically running
              'run-modified-tests --retry' to winnow the list of failure down.
  --retry FILE  like --retry, but use the tests listed in FILE instead of the
                cached file from the last testrun.

  --save      default: --save
  --nosave    don't update the test result cache with the results of this run
USAGE
}
